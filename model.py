import tensorflow as tf
import pathlib
import numpy as np

from tensorflow.keras import layers
from tensorflow.keras import losses

#Set Parameters
path = str(pathlib.Path.home()) + "/School/Comps/"
train_dir = str(pathlib.Path.home()) + "/School/Comps/dataset/train/"
train_dataset_dir = path + "dataset/train_save"
val_dataset_dir = path + "/dataset/val_save"
model_dir = path + "models/model"
checkpoint_path = path + "training_1/cp.ckpt"
batch_size = 12
epochs = 25


#Get dataset
train_ds = tf.data.experimental.load(train_dataset_dir)
val_ds = tf.data.experimental.load(val_dataset_dir)

train_text = [a[0] for a in train_ds.as_numpy_iterator()]
train_label = [a[1] for a in train_ds.as_numpy_iterator()]


#Get Input_Size
input_shape = (tf.shape(train_text[0])[1],)


#Normalization layers
norm = layers.Normalization()
norm.adapt(train_ds)
#THE model
test_model = tf.keras.models.Sequential([
    layers.Input(input_shape,batch_size=batch_size),
    norm,
    layers.Dense(50,activation="relu"),
    layers.Dense(9),
])
test_model.compile(
    loss=losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer="adam",
    metrics=["accuracy"]
)


#Save Model Weights
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                 save_weights_only=True,
                                                 verbose=1)
#Train Model
test_model.summary()
test_model.fit(train_ds,epochs=epochs,callbacks=[cp_callback])

#test = test_model.fit(train_text,train_label,batch_size=batch_size,epochs=epochs)
#test_model.save(model_dir)
