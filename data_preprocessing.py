import tensorflow as tf
import settings
import re

# parameters
parameters = settings.get_parameters()
train_dir = parameters["actual_data_path"]
train_dataset_dir = parameters["train_save_path"]
val_dataset_dir = parameters["val_save_path"]
test_dataset_dir = parameters["test_save_path"]
batch_size = settings.batch_size
seed = settings.seed
max_sequence = parameters["max_sequence"]
max_len = max_sequence * 16

#Methods

def hextoint(hex):
    """
    Convert hex to int
    Rescale from [0,255] to [0,1]
    pad to max_sequence
    """
    hexstr = hex.numpy().decode("ascii")
    hexarray = re.sub(r'[^\w\s]', '', hexstr).split()
    newtensor = tf.convert_to_tensor([int(x, 16) for x in hexarray], dtype=tf.float32)
    tf.math.divide(newtensor, 255)
    newtensor = tf.keras.preprocessing.sequence.pad_sequences([newtensor],
                                                  dtype="float32",
                                                  maxlen=max_len,
                                                  padding="post",
                                                  truncating="post")

    newtensor = tf.reshape(newtensor,[max_sequence,16])
    """
    shape = tf.shape(newtensor)
    numtopad = 16 - shape % 16
    if(numtopad != 16):
        newtensor = tf.concat([newtensor,tf.zeros(numtopad,dtype=tf.float32)],0)
        shape = shape + numtopad
    newshape = [tf.cast((shape/16)[0],dtype=tf.int32),tf.constant(16)]
    """
    return newtensor

def func(x,y):
    return (tf.py_function(hextoint,[x],Tout=tf.float32),y)


# create dataset
raw_train_set = tf.keras.utils.text_dataset_from_directory(
    train_dir,
    batch_size=batch_size,
    validation_split=0.2,
    subset="training",
    seed=seed
)

raw_val_set = tf.keras.utils.text_dataset_from_directory(
    train_dir,
    batch_size=batch_size,
    validation_split=0.2,
    subset="validation",
    seed=seed
)

split = int(raw_val_set.cardinality()/2)
raw_test_set = raw_val_set.take(split)
raw_val_set = raw_val_set.skip(split)

raw_train_set = raw_train_set.unbatch()
raw_val_set = raw_val_set.unbatch()
raw_test_set = raw_test_set.unbatch()


#add parrallel
train_ds = raw_train_set.map(func,num_parallel_calls=tf.data.AUTOTUNE)
val_ds = raw_val_set.map(func,num_parallel_calls=tf.data.AUTOTUNE)
test_ds = raw_test_set.map(func,num_parallel_calls=tf.data.AUTOTUNE)

#batch the dataset
train_ds = train_ds.batch(batch_size)
val_ds = val_ds.batch(batch_size)
test_ds = test_ds.batch(batch_size)

"""
#Pad the vectors
padded_shapes = (tf.TensorShape([None,None]), tf.TensorShape([]))
padding_values = (tf.constant(0,dtype="float32"), tf.constant(-11))
train_ds = train_ds.padded_batch(batch_size,padded_shapes=padded_shapes,padding_values=padding_values,drop_remainder=True)
val_ds = val_ds.padded_batch(batch_size,padded_shapes=padded_shapes,padding_values=padding_values,drop_remainder=True)


#Pad all
padded_shapes = (tf.TensorShape([None,None,None]), tf.TensorShape([None]))
padding_values = (tf.constant(0,dtype="float32"), tf.constant(-11))
train_ds = train_ds.padded_batch(batch_size,padded_shapes=padded_shapes,padding_values=padding_values)
train_ds = train_ds.unbatch()
val_ds = val_ds.padded_batch(batch_size,padded_shapes=padded_shapes,padding_values=padding_values)
val_ds = val_ds.unbatch()

max_sequence = max(tf.shape(train_ds.take(1).get_single_element()[0])[1],tf.shape(val_ds.take(1).get_single_element()[0])[1])

padded_shapes = (tf.TensorShape([None,max_sequence,None]), tf.TensorShape([None]))
train_ds = train_ds.padded_batch(batch_size,padded_shapes=padded_shapes,padding_values=padding_values)
train_ds = train_ds.unbatch()
val_ds = val_ds.padded_batch(batch_size,padded_shapes=padded_shapes,padding_values=padding_values)
val_ds = val_ds.unbatch()

"""

#Prefetch Input Pipeline Optimization
train_ds = train_ds.prefetch(tf.data.AUTOTUNE)
val_ds = val_ds.prefetch(tf.data.AUTOTUNE)
test_ds = test_ds.prefetch(tf.data.AUTOTUNE)

#Save Dataset
tf.data.experimental.save(train_ds,train_dataset_dir)
print("Saving train set...")
tf.data.experimental.save(val_ds,val_dataset_dir)
print("Saving validation set...")
tf.data.experimental.save(test_ds,test_dataset_dir)
print("Saving test set...")

