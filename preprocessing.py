import tensorflow as tf
import setting
import re

# parameters
parameters = setting.get_parameters()
train_dir = parameters["actual_data_path"]
train_dataset_dir = parameters["train_dataset_path"]
val_dataset_dir = parameters["val_save_path"]
batch_size = setting.batch_size
seed = setting.seed


#Methods
"""
def int_vectorize_byte(byte, label):
    byte = tf.expand_dims(byte, -1)
    return int_vectorize_layer(byte), label
"""

def hextoint(hex):
    hexstr = hex.numpy().decode("ascii")
    hexarray = re.sub(r'[^\w\s]', '', hexstr).split()
    newtensor = tf.convert_to_tensor([int(x, 16) for x in hexarray], dtype=tf.float32)
    tf.math.divide(newtensor, 255)
    shape = tf.shape(newtensor)
    numtopad = 16 - shape % 16
    if(numtopad != 16):
        newtensor = tf.concat([newtensor,tf.zeros(numtopad,dtype=tf.float32)],0)
        shape = shape + numtopad
    newshape = [tf.cast((shape/16)[0],dtype=tf.int32),tf.constant(16)]
    newtensor = tf.reshape(newtensor,newshape)
    return newtensor

def func(x,y):
    return (tf.py_function(hextoint,[x],Tout=tf.float32),y)

def pad_all_batch(dataset):
    for batch in dataset.as_numpy_iterator():
        values = batch[0]
        print(values.shape)

# create dataset
train_set = tf.keras.utils.text_dataset_from_directory(
    train_dir,
    batch_size=batch_size,
    validation_split=0.2,
    subset="training",
    seed=seed
)

val_set = tf.keras.utils.text_dataset_from_directory(
    train_dir,
    batch_size=batch_size,
    validation_split=0.2,
    subset="validation",
    seed=seed
)

""""
#create vocabulary list(0-255 in hex)
vocab_set = []
for i in range(255):
    vocab = hex(i)[2:]
    if(i < 16):
        vocab = "0" + vocab
    vocab_set.append(vocab)
"""



"""
int_vectorize_layer = layers.TextVectorization(
    max_tokens=len(vocab_set)+2,
    standardize='lower_and_strip_punctuation',
    split='whitespace',
    output_mode="int",

    vocabulary=vocab_set
)
"""

train_set = train_set.unbatch()
val_set = val_set.unbatch()


train_ds = train_set.map(func)
val_ds = val_set.map(func)


#Pad the vectors
padded_shapes = (tf.TensorShape([None,None]), tf.TensorShape([]))
padding_values = (tf.constant(0,dtype="float32"), tf.constant(-11))
train_ds = train_ds.padded_batch(batch_size,padded_shapes=padded_shapes,padding_values=padding_values,drop_remainder=True)
val_ds = val_ds.padded_batch(batch_size,padded_shapes=padded_shapes,padding_values=padding_values,drop_remainder=True)


#Pad all
padded_shapes = (tf.TensorShape([None,None,None]), tf.TensorShape([None]))
padding_values = (tf.constant(0,dtype="float32"), tf.constant(-11))
train_ds = train_ds.padded_batch(batch_size,padded_shapes=padded_shapes,padding_values=padding_values)
train_ds = train_ds.unbatch()


max_sequence = tf.shape(train_ds.take(1).get_single_element()[0])[1]

padded_shapes = (tf.TensorShape([None,max_sequence,None]), tf.TensorShape([None]))
val_ds = val_ds.padded_batch(batch_size,padded_shapes=padded_shapes,padding_values=padding_values)
val_ds = val_ds.unbatch()



#Prefetch Input Pipeline Optimization
train_ds = train_ds.prefetch(tf.data.AUTOTUNE)
val_ds = val_ds.prefetch(tf.data.AUTOTUNE)

#Save Dataset
tf.data.experimental.save(train_ds,train_dataset_dir)
tf.data.experimental.save(val_ds,val_dataset_dir)

