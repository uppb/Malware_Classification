import tensorflow as tf
import pathlib

from tensorflow.keras import layers

# parameters
train_dir = str(pathlib.Path.home()) + "/School/Comps/dataset/train/"
train_dataset_dir = str(pathlib.Path.home()) + "/School/Comps/dataset/train_save"
val_dataset_dir = str(pathlib.Path.home()) + "/School/Comps/dataset/val_save"
batch_size = 12
seed = 42

#Methods
def int_vectorize_byte(byte, label):
    byte = tf.expand_dims(byte, -1)
    return int_vectorize_layer(byte), label


# create dataset
train_set = tf.keras.utils.text_dataset_from_directory(
    train_dir,
    batch_size=batch_size,
    validation_split=0.2,
    subset="training",
    seed=seed
)

val_set = tf.keras.utils.text_dataset_from_directory(
    train_dir,
    batch_size=batch_size,
    validation_split=0.2,
    subset="validation",
    seed=seed
)

#create vocabulary list(0-255 in hex)
vocab_set = []
for i in range(255):
    vocab = hex(i)[2:]
    if(i < 16):
        vocab = "0" + vocab
    vocab_set.append(vocab)


"""
#create dataset with only data
raw_bytes = train_set.map(lambda text, labels:text)

#vectorize data
raw_bytes = np.asarray(list(raw_bytes.as_numpy_iterator()))
raw_bytes = np.hstack(raw_bytes) #flatten out batches
for i in range(len(raw_bytes)):
    raw_bytes[i] = raw_bytes[i].decode("utf-8")
"""

#tokenize the data
"""
tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n',split=' ',char_level = False,lower=True,num_words=256)
tokenizer.fit_on_texts(train_bytes)
train_bytes_token = tokenizer.texts_to_sequences(train_bytes)
"""
int_vectorize_layer = layers.TextVectorization(
    max_tokens=len(vocab_set)+2,
    standardize='lower_and_strip_punctuation',
    split='whitespace',
    output_mode="int",

    vocabulary=vocab_set
)

#Bytes Vectorization
train_ds = train_set.map(int_vectorize_byte)
val_ds = val_set.map(int_vectorize_byte)

#Pad the vectors
padded_shapes = (tf.TensorShape([None,None]), tf.TensorShape([None]))
padding_values = (tf.constant(0,dtype="int64"), tf.constant(-111))
train_ds = train_ds.padded_batch(batch_size,padded_shapes=padded_shapes,padding_values=padding_values)
train_ds = train_ds.unbatch()
val_ds = val_ds.padded_batch(batch_size,padded_shapes=padded_shapes,padding_values=padding_values)
val_ds = val_ds.unbatch()


#Prefetch Input Pipeline Optimization
train_ds = train_ds.prefetch(tf.data.AUTOTUNE)
val_ds = val_ds.prefetch(tf.data.AUTOTUNE)

#Save Dataset
tf.data.experimental.save(train_ds,train_dataset_dir)
tf.data.experimental.save(val_ds,val_dataset_dir)









