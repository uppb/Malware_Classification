import tensorflow as tf


from tensorflow.keras import layers,losses,optimizers
from tensorflow.keras.models import Model

def convBlock(input,filters,kernel_size,strides=1,pool_size=0,batch_norm=False):
    output = input
    output = layers.Conv1D(filters,kernel_size,strides=strides,padding="valid",activation="relu")(output)
    if batch_norm:
        output = layers.BatchNormalization()(output)
    if(pool_size > 1):
        output = layers.MaxPool1D(pool_size)(output)
    return output


def cnn(input_shape):
    inputs = layers.Input(input_shape)
    x = layers.Masking(tf.constant(0,dtype=tf.float32))(inputs)
    x = convBlock(x,filters=16,kernel_size=64,strides=2,pool_size=4)
    x = convBlock(x,filters=16,kernel_size=64, strides=2, pool_size=4)
    #x = convBlock(x,32,64,2,8)
    #x = convBlock(x,64,32,2,4)

    x = layers.GlobalMaxPooling1D() (x)
    x = layers.Dense(64, activation="relu") (x)
    x = layers.Dropout(0.5)(x)
    outputs = layers.Dense(9) (x)

    model = Model(inputs=inputs, outputs=outputs)

    model.compile(
        loss=losses.SparseCategoricalCrossentropy(from_logits=True),
        optimizer="adam",
        metrics=["accuracy"]
    )

    return model

def cnn_attention(input_shape):
    inputs = layers.Input(input_shape)
    x = layers.Masking(tf.constant(0,dtype=tf.float32))(inputs)

    #CONV
    conv1 = convBlock(x,filters=216,kernel_size=4,strides=1,pool_size=8)
    conv2 = convBlock(conv1,filters=432,kernel_size=2,strides=1,pool_size=8)
    conv3 = convBlock(conv2,filters=576,kernel_size=2,strides=1,pool_size=4)

    fc1 = layers.Dense(576, activation="selu") (conv3)
    fc2 = layers.Dense(576, activation="selu") (fc1)
    fc3 = layers.Dense(576, activation="selu") (fc2)
    fc4 = layers.Dense(432, activation="selu") (fc3)

    #ATTENTION
    attention1 = layers.Attention()([conv2, fc4])
    attention2 = layers.Attention()([conv3, fc3])
    attention1 = layers.GlobalAveragePooling1D()(attention1)
    attention2 = layers.GlobalAveragePooling1D()(attention2)
    attention = layers.Concatenate(axis=1)([attention1,attention2])

    #DENSE
    outputs = layers.Dropout(0.5)(attention)
    outputs = layers.Dense(9)(outputs)
    model = Model(inputs=inputs, outputs=outputs)

    model.compile(
        loss=losses.SparseCategoricalCrossentropy(from_logits=True),
        optimizer=optimizers.Adam(),
        metrics=["accuracy"]
    )

    return model