import tensorflow as tf


from tensorflow.keras import layers,losses,optimizers
from tensorflow.keras.models import Model

def convBlock(input,filters,kernel_size,strides=1,pool_size=0,batch_norm=False):
    output = input
    output = layers.Conv1D(filters,kernel_size,strides=strides,padding="valid",activation="relu")(output)
    if batch_norm:
        output = layers.BatchNormalization()(output)
    if(pool_size > 1):
        output = layers.MaxPool1D(pool_size)(output)
    return output


def cnn(input_shape):
    inputs = layers.Input(input_shape)
    x = layers.Masking(tf.constant(0,dtype=tf.float32))(inputs)
    conv1 = convBlock(x,filters=128,kernel_size=4,strides=1,pool_size=8)
    conv2 = convBlock(conv1,filters=192,kernel_size=2,strides=1,pool_size=8)
    conv3 = convBlock(conv2,filters=384,kernel_size=2,strides=1,pool_size=4)

    pool = layers.GlobalAveragePooling1D() (conv3)
    fc1 = layers.Dense(384, activation="selu") (pool)
    fc2 = layers.Dense(384, activation="selu") (fc1)
    fc3 = layers.Dense(384, activation="selu") (fc2)
    fc4 = layers.Dense(192, activation="selu") (fc3)
    x = layers.Dropout(0.5)(fc4)
    outputs = layers.Dense(9) (x)

    model = Model(inputs=inputs, outputs=outputs)

    model.compile(
        loss=losses.SparseCategoricalCrossentropy(from_logits=True),
        optimizer="adam",
        metrics=["accuracy"]
    )

    return model

def cnn_attention(input_shape):
    inputs = layers.Input(input_shape)
    x = layers.Masking(tf.constant(0,dtype=tf.float32))(inputs)

    #CONV
    conv1 = convBlock(x,filters=128,kernel_size=4,strides=1,pool_size=8)
    conv2 = convBlock(conv1,filters=192,kernel_size=2,strides=1,pool_size=8)
    conv3 = convBlock(conv2,filters=384,kernel_size=2,strides=1,pool_size=4)

    fc1 = layers.Dense(384, activation="selu") (conv3)
    fc2 = layers.Dense(384, activation="selu") (fc1)
    fc3 = layers.Dense(384, activation="selu") (fc2)
    fc4 = layers.Dense(192, activation="selu") (fc3)

    #ATTENTION
    attention1 = layers.Attention()([conv2, fc4])
    attention2 = layers.Attention()([conv3, fc3])
    attention1 = layers.GlobalAveragePooling1D()(attention1)
    attention2 = layers.GlobalAveragePooling1D()(attention2)
    attention = layers.Concatenate(axis=1)([attention1,attention2])

    #DENSE
    outputs = layers.Dropout(0.5)(attention)
    outputs = layers.Dense(9)(outputs)
    model = Model(inputs=inputs, outputs=outputs)

    model.compile(
        loss=losses.SparseCategoricalCrossentropy(from_logits=True),
        optimizer=optimizers.Adam(),
        metrics=["accuracy"]
    )

    return model