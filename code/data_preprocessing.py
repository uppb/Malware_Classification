import tensorflow as tf
import settings
import re

# parameters
parameters = settings.get_parameters()
train_dir = parameters["actual_data_path"]
train_dataset_dir = parameters["train_save_path"]
val_dataset_dir = parameters["val_save_path"]
test_dataset_dir = parameters["test_save_path"]
batch_size = settings.batch_size
seed = settings.seed
max_sequence = parameters["max_sequence_truncated"]
max_len = max_sequence * 16

#Methods

def hextoint(hex):
    """
    Convert hex to int
    Rescale from [0,255] to [0,1]
    pad to max_sequence
    """

    hexstr = hex.numpy().decode("ascii")
    hexarray = re.sub(r'[^\w\s]', '', hexstr).split()
    newtensor = tf.convert_to_tensor([int(x, 16) for x in hexarray], dtype=tf.float32)
    tf.math.divide(newtensor, 255)
    newtensor = tf.keras.preprocessing.sequence.pad_sequences([newtensor],
                                                  dtype="float32",
                                                  maxlen=max_len,
                                                  padding="post",
                                                  truncating="post")

    newtensor = tf.reshape(newtensor,[max_sequence,16])
    return newtensor

def func(x,y):
    return (tf.py_function(hextoint,[x],Tout=tf.float32),y)


# create dataset
raw_train_set = tf.keras.utils.text_dataset_from_directory(
    train_dir,
    batch_size=batch_size,
    validation_split=0.2,
    subset="training",
    seed=seed
)

raw_val_set = tf.keras.utils.text_dataset_from_directory(
    train_dir,
    batch_size=batch_size,
    validation_split=0.2,
    subset="validation",
    seed=seed
)

split = int(raw_val_set.cardinality()/2)
raw_test_set = raw_val_set.take(split)
raw_val_set = raw_val_set.skip(split)


raw_train_set = raw_train_set.unbatch()
raw_val_set = raw_val_set.unbatch()
raw_test_set = raw_test_set.unbatch()



#pad all data
train_ds = raw_train_set.map(func,num_parallel_calls=tf.data.AUTOTUNE)
val_ds = raw_val_set.map(func,num_parallel_calls=tf.data.AUTOTUNE)
test_ds = raw_test_set.map(func,num_parallel_calls=tf.data.AUTOTUNE)


#Prefetch Input Pipeline Optimization
train_ds = train_ds.prefetch(tf.data.AUTOTUNE)
val_ds = val_ds.prefetch(tf.data.AUTOTUNE)
test_ds = test_ds.prefetch(tf.data.AUTOTUNE)

#Save Dataset
tf.data.experimental.save(train_ds,train_dataset_dir)
print("Saving train set...")
tf.data.experimental.save(val_ds,val_dataset_dir)
print("Saving validation set...")
tf.data.experimental.save(test_ds,test_dataset_dir)
print("Saving test set...")

